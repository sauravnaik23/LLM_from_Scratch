{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_dataloader, GPTTokenizer\n",
    "from model import (GPTModel, inference, token_ids_to_text, text_to_token_ids, generate)\n",
    "from config import CUSTOM_GPT_CONFIG, GPT2_CONFIG\n",
    "from torchinfo import summary\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from rich import print as pprint\n",
    "from torch import tensor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"Merry Christmas!!\",\"Whos is smart\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context_length': 1024,\n",
       " 'emb_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'drop_rate': 0.1,\n",
       " 'qkv_bias': True}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUSTOM_GPT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(CUSTOM_GPT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "GPTModel                                 --\n",
       "├─Embedding: 1-1                         38,597,376\n",
       "├─Embedding: 1-2                         786,432\n",
       "├─Dropout: 1-3                           --\n",
       "├─Sequential: 1-4                        --\n",
       "│    └─TransformerBlock: 2-1             --\n",
       "│    │    └─MultiHeadAttention: 3-1      2,362,368\n",
       "│    │    └─FeedForward: 3-2             4,722,432\n",
       "│    │    └─LayerNorm: 3-3               1,536\n",
       "│    │    └─LayerNorm: 3-4               1,536\n",
       "│    │    └─Dropout: 3-5                 --\n",
       "│    └─TransformerBlock: 2-2             --\n",
       "│    │    └─MultiHeadAttention: 3-6      2,362,368\n",
       "│    │    └─FeedForward: 3-7             4,722,432\n",
       "│    │    └─LayerNorm: 3-8               1,536\n",
       "│    │    └─LayerNorm: 3-9               1,536\n",
       "│    │    └─Dropout: 3-10                --\n",
       "│    └─TransformerBlock: 2-3             --\n",
       "│    │    └─MultiHeadAttention: 3-11     2,362,368\n",
       "│    │    └─FeedForward: 3-12            4,722,432\n",
       "│    │    └─LayerNorm: 3-13              1,536\n",
       "│    │    └─LayerNorm: 3-14              1,536\n",
       "│    │    └─Dropout: 3-15                --\n",
       "│    └─TransformerBlock: 2-4             --\n",
       "│    │    └─MultiHeadAttention: 3-16     2,362,368\n",
       "│    │    └─FeedForward: 3-17            4,722,432\n",
       "│    │    └─LayerNorm: 3-18              1,536\n",
       "│    │    └─LayerNorm: 3-19              1,536\n",
       "│    │    └─Dropout: 3-20                --\n",
       "│    └─TransformerBlock: 2-5             --\n",
       "│    │    └─MultiHeadAttention: 3-21     2,362,368\n",
       "│    │    └─FeedForward: 3-22            4,722,432\n",
       "│    │    └─LayerNorm: 3-23              1,536\n",
       "│    │    └─LayerNorm: 3-24              1,536\n",
       "│    │    └─Dropout: 3-25                --\n",
       "│    └─TransformerBlock: 2-6             --\n",
       "│    │    └─MultiHeadAttention: 3-26     2,362,368\n",
       "│    │    └─FeedForward: 3-27            4,722,432\n",
       "│    │    └─LayerNorm: 3-28              1,536\n",
       "│    │    └─LayerNorm: 3-29              1,536\n",
       "│    │    └─Dropout: 3-30                --\n",
       "│    └─TransformerBlock: 2-7             --\n",
       "│    │    └─MultiHeadAttention: 3-31     2,362,368\n",
       "│    │    └─FeedForward: 3-32            4,722,432\n",
       "│    │    └─LayerNorm: 3-33              1,536\n",
       "│    │    └─LayerNorm: 3-34              1,536\n",
       "│    │    └─Dropout: 3-35                --\n",
       "│    └─TransformerBlock: 2-8             --\n",
       "│    │    └─MultiHeadAttention: 3-36     2,362,368\n",
       "│    │    └─FeedForward: 3-37            4,722,432\n",
       "│    │    └─LayerNorm: 3-38              1,536\n",
       "│    │    └─LayerNorm: 3-39              1,536\n",
       "│    │    └─Dropout: 3-40                --\n",
       "│    └─TransformerBlock: 2-9             --\n",
       "│    │    └─MultiHeadAttention: 3-41     2,362,368\n",
       "│    │    └─FeedForward: 3-42            4,722,432\n",
       "│    │    └─LayerNorm: 3-43              1,536\n",
       "│    │    └─LayerNorm: 3-44              1,536\n",
       "│    │    └─Dropout: 3-45                --\n",
       "│    └─TransformerBlock: 2-10            --\n",
       "│    │    └─MultiHeadAttention: 3-46     2,362,368\n",
       "│    │    └─FeedForward: 3-47            4,722,432\n",
       "│    │    └─LayerNorm: 3-48              1,536\n",
       "│    │    └─LayerNorm: 3-49              1,536\n",
       "│    │    └─Dropout: 3-50                --\n",
       "│    └─TransformerBlock: 2-11            --\n",
       "│    │    └─MultiHeadAttention: 3-51     2,362,368\n",
       "│    │    └─FeedForward: 3-52            4,722,432\n",
       "│    │    └─LayerNorm: 3-53              1,536\n",
       "│    │    └─LayerNorm: 3-54              1,536\n",
       "│    │    └─Dropout: 3-55                --\n",
       "│    └─TransformerBlock: 2-12            --\n",
       "│    │    └─MultiHeadAttention: 3-56     2,362,368\n",
       "│    │    └─FeedForward: 3-57            4,722,432\n",
       "│    │    └─LayerNorm: 3-58              1,536\n",
       "│    │    └─LayerNorm: 3-59              1,536\n",
       "│    │    └─Dropout: 3-60                --\n",
       "├─LayerNorm: 1-5                         1,536\n",
       "├─Linear: 1-6                            38,597,376\n",
       "=================================================================\n",
       "Total params: 163,037,184\n",
       "Trainable params: 163,037,184\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163037184"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = 0\n",
    "for param in model.parameters():\n",
    "    total_params += param.numel()\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (50257) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# prompts = [\"Who is the\"]\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m res \u001b[39m=\u001b[39m    generate(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      3\u001b[0m          tokenizer\u001b[39m=\u001b[39;49mGPTTokenizer,\n\u001b[1;32m      4\u001b[0m          max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m          temperature\u001b[39m=\u001b[39;49m \u001b[39m0.8\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m          DEVICE \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m          prompts\u001b[39m=\u001b[39;49mprompts,\n\u001b[1;32m      8\u001b[0m          context_size\u001b[39m=\u001b[39;49mCUSTOM_GPT_CONFIG[\u001b[39m'\u001b[39;49m\u001b[39mcontext_length\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/Desktop/LLM from scratch/model.py:257\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompts, context_size, model, tokenizer, temperature, top_K, max_new_tokens, eos_id, DEVICE)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(prompts:\u001b[39mlist\u001b[39m,\n\u001b[1;32m    248\u001b[0m              context_size:\u001b[39mint\u001b[39m,\n\u001b[1;32m    249\u001b[0m              model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m              eos_id \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    255\u001b[0m              DEVICE \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    256\u001b[0m     input_tokens \u001b[39m=\u001b[39m text_to_token_ids(prompts, GPTTokenizer)\n\u001b[0;32m--> 257\u001b[0m     out_tokens \u001b[39m=\u001b[39m inference(model \u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    258\u001b[0m                            idx \u001b[39m=\u001b[39;49m input_tokens,\n\u001b[1;32m    259\u001b[0m                            context_size\u001b[39m=\u001b[39;49mcontext_size,\n\u001b[1;32m    260\u001b[0m                            temperature \u001b[39m=\u001b[39;49m temperature,\n\u001b[1;32m    261\u001b[0m                            top_k \u001b[39m=\u001b[39;49m top_K,\n\u001b[1;32m    262\u001b[0m                            max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens,\n\u001b[1;32m    263\u001b[0m                            DEVICE\u001b[39m=\u001b[39;49mDEVICE,\n\u001b[1;32m    264\u001b[0m                            eos_id\u001b[39m=\u001b[39;49meos_id)\n\u001b[1;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m token_ids_to_text(out_tokens, tokenizer\u001b[39m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/Desktop/LLM from scratch/model.py:230\u001b[0m, in \u001b[0;36minference\u001b[0;34m(model, idx, max_new_tokens, context_size, temperature, top_k, eos_id, DEVICE)\u001b[0m\n\u001b[1;32m    228\u001b[0m     top_logits, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtopk(logits, top_k)\n\u001b[1;32m    229\u001b[0m     min_val \u001b[39m=\u001b[39m top_logits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 230\u001b[0m     logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(logits \u001b[39m<\u001b[39;49m min_val, torch\u001b[39m.\u001b[39mtensor(\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39mto(logits\u001b[39m.\u001b[39mdevice), logits)\n\u001b[1;32m    231\u001b[0m \u001b[39m# temperature scaling\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m temperature \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (50257) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# prompts = [\"Who is the\"]\n",
    "res =    generate(model=model,\n",
    "         tokenizer=GPTTokenizer,\n",
    "         max_new_tokens=30,\n",
    "         temperature= 0.8,\n",
    "         DEVICE = 'cpu',\n",
    "         prompts=prompts,\n",
    "         context_size=CUSTOM_GPT_CONFIG['context_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Who is theanasia town Nathanieliveryindustrial cleared empathy tongues Sergeant Gift 333Resource._ 106 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">detractorsml amino half restart Obviously register membranes Jarvis Servicesinatorsstrengthreau draincollection </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Stub'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'Who is theanasia town Nathanieliveryindustrial cleared empathy tongues Sergeant Gift 333Resource._ 106 \u001b[0m\n",
       "\u001b[32mdetractorsml amino half restart Obviously register membranes Jarvis Servicesinatorsstrengthreau draincollection \u001b[0m\n",
       "\u001b[32mStub'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Response <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> : \n",
       "\n",
       " Who is the Ticket姫 Namhend Frozen Clarke bachelor notes objectionable empt preferences journalismÃÂÃÂ greatness \n",
       "Peyton multiplying Usari lastedOSE Pharm Ple insisting totalsuary Piercingnesty unn� overturned\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Response \u001b[1;36m1\u001b[0m : \n",
       "\n",
       " Who is the Ticket姫 Namhend Frozen Clarke bachelor notes objectionable empt preferences journalismÃÂÃÂ greatness \n",
       "Peyton multiplying Usari lastedOSE Pharm Ple insisting totalsuary Piercingnesty unn� overturned\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, response in enumerate(res,1):\n",
    "    pprint(f\"Response {idx} : \\n\\n {response}\", end = '\\n\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting OPEN-AI Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "model_size = \"124M\"\n",
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: downloaded_weights/124M/checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: downloaded_weights/124M/encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: downloaded_weights/124M/hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: downloaded_weights/124M/model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: downloaded_weights/124M/model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: downloaded_weights/124M/model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: downloaded_weights/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"downloaded_weights\"\n",
    "settings, params = download_and_load_gpt2(model_size=model_size,\n",
    "                                          models_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (50257) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m prompts \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mMarry had a little lamb\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> 3\u001b[0m res \u001b[39m=\u001b[39m    generate(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      4\u001b[0m          tokenizer\u001b[39m=\u001b[39;49mGPTTokenizer,\n\u001b[1;32m      5\u001b[0m          max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m          temperature\u001b[39m=\u001b[39;49m \u001b[39m0.9\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m          DEVICE \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m          prompts\u001b[39m=\u001b[39;49mprompts,\n\u001b[1;32m      9\u001b[0m          context_size\u001b[39m=\u001b[39;49mCUSTOM_GPT_CONFIG[\u001b[39m'\u001b[39;49m\u001b[39mcontext_length\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     10\u001b[0m          top_K\u001b[39m=\u001b[39;49m \u001b[39m50\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m          eos_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m<|endoftext|>\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m idx, response \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(res,\u001b[39m1\u001b[39m):\n\u001b[1;32m     14\u001b[0m     pprint(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResponse \u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m : \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, end \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/LLM from scratch/model.py:257\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompts, context_size, model, tokenizer, temperature, top_K, max_new_tokens, eos_id, DEVICE)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(prompts:\u001b[39mlist\u001b[39m,\n\u001b[1;32m    248\u001b[0m              context_size:\u001b[39mint\u001b[39m,\n\u001b[1;32m    249\u001b[0m              model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m              eos_id \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    255\u001b[0m              DEVICE \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    256\u001b[0m     input_tokens \u001b[39m=\u001b[39m text_to_token_ids(prompts, GPTTokenizer)\n\u001b[0;32m--> 257\u001b[0m     out_tokens \u001b[39m=\u001b[39m inference(model \u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    258\u001b[0m                            idx \u001b[39m=\u001b[39;49m input_tokens,\n\u001b[1;32m    259\u001b[0m                            context_size\u001b[39m=\u001b[39;49mcontext_size,\n\u001b[1;32m    260\u001b[0m                            temperature \u001b[39m=\u001b[39;49m temperature,\n\u001b[1;32m    261\u001b[0m                            top_k \u001b[39m=\u001b[39;49m top_K,\n\u001b[1;32m    262\u001b[0m                            max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens,\n\u001b[1;32m    263\u001b[0m                            DEVICE\u001b[39m=\u001b[39;49mDEVICE,\n\u001b[1;32m    264\u001b[0m                            eos_id\u001b[39m=\u001b[39;49meos_id)\n\u001b[1;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m token_ids_to_text(out_tokens, tokenizer\u001b[39m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/Desktop/LLM from scratch/model.py:230\u001b[0m, in \u001b[0;36minference\u001b[0;34m(model, idx, max_new_tokens, context_size, temperature, top_k, eos_id, DEVICE)\u001b[0m\n\u001b[1;32m    228\u001b[0m     top_logits, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtopk(logits, top_k)\n\u001b[1;32m    229\u001b[0m     min_val \u001b[39m=\u001b[39m top_logits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 230\u001b[0m     logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(logits \u001b[39m<\u001b[39;49m min_val, torch\u001b[39m.\u001b[39mtensor(\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39mto(logits\u001b[39m.\u001b[39mdevice), logits)\n\u001b[1;32m    231\u001b[0m \u001b[39m# temperature scaling\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m temperature \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (50257) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "prompts = [\"Marry had a little lamb\"] * 2\n",
    "model.eval()\n",
    "res =    generate(model=model,\n",
    "         tokenizer=GPTTokenizer,\n",
    "         max_new_tokens=100,\n",
    "         temperature= 0.9,\n",
    "         DEVICE = 'cpu',\n",
    "         prompts=prompts,\n",
    "         context_size=CUSTOM_GPT_CONFIG['context_length'],\n",
    "         top_K= 50,\n",
    "         eos_id=\"<|endoftext|>\")\n",
    "\n",
    "for idx, response in enumerate(res,1):\n",
    "    pprint(f\"Response {idx} : \\n\\n {response}\", end = '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
