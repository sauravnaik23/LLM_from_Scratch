{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rich import print as pprint\n",
    "\n",
    "from dataset import GPTTokenizer\n",
    "import torch\n",
    "from torch import tensor, stack, nonzero\n",
    "from torch.nn import functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_df = pd.read_json('instruction-data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>Classify the following items: pine tree, quart...</td>\n",
       "      <td></td>\n",
       "      <td>Plants: Pine tree\\nMinerals: Quartz\\nAnimals: ...</td>\n",
       "      <td>(Below is an instruction that describes the ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Generate a sentence using the word 'harmony'.</td>\n",
       "      <td></td>\n",
       "      <td>The choir's voices blended in perfect harmony.</td>\n",
       "      <td>(Below is an instruction that describes the ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Provide a word that rhymes with \"care.\"</td>\n",
       "      <td></td>\n",
       "      <td>A word that rhymes with \"care\" is \"fare.\"</td>\n",
       "      <td>(Below is an instruction that describes the ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>Define the term 'kinetic energy'.</td>\n",
       "      <td></td>\n",
       "      <td>Kinetic energy is the energy that an object po...</td>\n",
       "      <td>(Below is an instruction that describes the ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>What is the normal body temperature in Celsius?</td>\n",
       "      <td></td>\n",
       "      <td>The normal body temperature is approximately 3...</td>\n",
       "      <td>(Below is an instruction that describes the ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>Translate 'Where are you from?' into French.</td>\n",
       "      <td></td>\n",
       "      <td>The French translation of 'Where are you from?...</td>\n",
       "      <td>(Below is an instruction that describes the ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>What is the abbreviation for 'Master of Busine...</td>\n",
       "      <td></td>\n",
       "      <td>The abbreviation for 'Master of Business Admin...</td>\n",
       "      <td>(Below is an instruction that describes the ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>Create a sentence using the word 'inevitable'.</td>\n",
       "      <td></td>\n",
       "      <td>The confrontation was inevitable given the cir...</td>\n",
       "      <td>(Below is an instruction that describes the ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>Rewrite this statement as an imperative sentence.</td>\n",
       "      <td>You should finish your assignment.</td>\n",
       "      <td>Finish your assignment.</td>\n",
       "      <td>(Below is an instruction that describes the ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>Name the process by which plants absorb water ...</td>\n",
       "      <td></td>\n",
       "      <td>The process by which plants absorb water throu...</td>\n",
       "      <td>(Below is an instruction that describes the ta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction  \\\n",
       "763   Classify the following items: pine tree, quart...   \n",
       "88        Generate a sentence using the word 'harmony'.   \n",
       "259             Provide a word that rhymes with \"care.\"   \n",
       "962                   Define the term 'kinetic energy'.   \n",
       "54      What is the normal body temperature in Celsius?   \n",
       "...                                                 ...   \n",
       "470        Translate 'Where are you from?' into French.   \n",
       "1014  What is the abbreviation for 'Master of Busine...   \n",
       "993      Create a sentence using the word 'inevitable'.   \n",
       "344   Rewrite this statement as an imperative sentence.   \n",
       "1067  Name the process by which plants absorb water ...   \n",
       "\n",
       "                                   input  \\\n",
       "763                                        \n",
       "88                                         \n",
       "259                                        \n",
       "962                                        \n",
       "54                                         \n",
       "...                                  ...   \n",
       "470                                        \n",
       "1014                                       \n",
       "993                                        \n",
       "344   You should finish your assignment.   \n",
       "1067                                       \n",
       "\n",
       "                                                 output  \\\n",
       "763   Plants: Pine tree\\nMinerals: Quartz\\nAnimals: ...   \n",
       "88       The choir's voices blended in perfect harmony.   \n",
       "259           A word that rhymes with \"care\" is \"fare.\"   \n",
       "962   Kinetic energy is the energy that an object po...   \n",
       "54    The normal body temperature is approximately 3...   \n",
       "...                                                 ...   \n",
       "470   The French translation of 'Where are you from?...   \n",
       "1014  The abbreviation for 'Master of Business Admin...   \n",
       "993   The confrontation was inevitable given the cir...   \n",
       "344                             Finish your assignment.   \n",
       "1067  The process by which plants absorb water throu...   \n",
       "\n",
       "                                              formatted  \n",
       "763   (Below is an instruction that describes the ta...  \n",
       "88    (Below is an instruction that describes the ta...  \n",
       "259   (Below is an instruction that describes the ta...  \n",
       "962   (Below is an instruction that describes the ta...  \n",
       "54    (Below is an instruction that describes the ta...  \n",
       "...                                                 ...  \n",
       "470   (Below is an instruction that describes the ta...  \n",
       "1014  (Below is an instruction that describes the ta...  \n",
       "993   (Below is an instruction that describes the ta...  \n",
       "344   (Below is an instruction that describes the ta...  \n",
       "1067  (Below is an instruction that describes the ta...  \n",
       "\n",
       "[220 rows x 4 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst_df.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Formtting the data\n",
    "def format_input(entry):\n",
    "    instruction_text = (f\"Below is an instruction that describes the task. \"\n",
    "                        f\"Write a response that appropriately completes the request\"\n",
    "                        f\"\\n\\n### Instruction:\\n{entry['instruction']}\")\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    response_text = f\"\\n\\n### Response:\\n\"\n",
    "\n",
    "    return instruction_text + input_text + response_text, entry['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name the process by which plants absorb water through their roots.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst_df.loc[1067]['instruction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_df['formatted'] = inst_df[['instruction','input','output']].apply(format_input, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alpaca Prompt style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Below is an instruction that describes the task. Write a response that appropriately completes the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">request\\n\\n### Instruction:\\nName a synonym for \"happiness.\"\\n\\n### Response:\\n'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'A synonym for \"happiness\" is \"joy.\"'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[32m'Below is an instruction that describes the task. Write a response that appropriately completes the \u001b[0m\n",
       "\u001b[32mrequest\\n\\n### Instruction:\\nName a synonym for \"happiness.\"\\n\\n### Response:\\n'\u001b[0m,\n",
       "    \u001b[32m'A synonym for \"happiness\" is \"joy.\"'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(inst_df['formatted'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = inst_df[:int(0.8 * len(inst_df))]\n",
    "test_data =  inst_df[int(0.8 * len(inst_df)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "# GPTTokenizer.allowed_special = \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsructionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data:pd.DataFrame, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        for _,row in self.data.iterrows():\n",
    "            input_, response = self.__alpaca_format_input(row)\n",
    "            formatted = input_ + response\n",
    "            encoded_text = tokenizer.encode(formatted)\n",
    "            self.encoded_texts.append(encoded_text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __alpaca_format_input(self,entry):\n",
    "        instruction_text = (f\"Below is an instruction that describes the task. \"\n",
    "                            f\"Write a response that appropriately completes the request\"\n",
    "                            f\"\\n\\n### Instruction:\\n{entry['instruction']}\")\n",
    "        input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "        response_text = f\"\\n\\n### Response:\\n\"\n",
    "        return instruction_text + input_text + response_text, entry['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, pad_token_id = 50256,\n",
    "                      ignore_index = -100, DEVICE=\"cpu\",\n",
    "                      allowed_max_len = None):\n",
    "\n",
    "    ## finding length + 1 longest sequence in the batch\n",
    "    batch_max_len =max(len(item)+1 for item in batch)\n",
    "    # PAD and prepare inputs\n",
    "    inputs_lst = []\n",
    "    targets_lst = []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item+=[pad_token_id]\n",
    "        padded = (new_item + [pad_token_id] * (batch_max_len - len(new_item)))\n",
    "        inputs = tensor(padded[:-1])\n",
    "        targets = tensor(padded[1:])\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "        mask = targets==pad_token_id\n",
    "        indices_to_replace = nonzero(mask).squeeze()\n",
    "        ## making extra padding as -100 so that it is ignored by cross_entropy loss\n",
    "        if indices_to_replace.numel()>1:\n",
    "            targets[indices_to_replace[1:]] = ignore_index\n",
    "        ## truncating length\n",
    "        if allowed_max_len is not None:\n",
    "            inputs = inputs[:allowed_max_len]\n",
    "            targets = targets[:allowed_max_len]\n",
    "    input_tensor = stack(inputs_lst).to(DEVICE)\n",
    "    targets_tensor = stack(targets_lst).to(DEVICE)\n",
    "    return {'x':input_tensor, 'y':targets_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = (\n",
    "            [1,3,4,5,6,7],   # Sequence 1\n",
    "            [4,2,3],         # Sequence 2\n",
    "            [0,8,1,2,9]      # Sequence 3\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[    1,     3,     4,     5,     6,     7],\n",
       "         [    4,     2,     3, 50256, 50256, 50256],\n",
       "         [    0,     8,     1,     2,     9, 50256]]),\n",
       " 'y': tensor([[    3,     4,     5,     6,     7, 50256],\n",
       "         [    2,     3, 50256,  -100,  -100,  -100],\n",
       "         [    8,     1,     2,     9, 50256,  -100]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "customzed_collate_function = partial(custom_collate_fn, DEVICE = 'cpu',\n",
    "                                     allowed_max_len = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 64\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_dataset = InsructionDataset(inst_df, GPTTokenizer)\n",
    "train_loader =DataLoader(train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         collate_fn= custom_collate_fn,\n",
    "                         shuffle= True,\n",
    "                         drop_last= True,\n",
    "                         num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     pprint(GPTTokenizer.decode_batch(batch['x'].numpy()), end = '\\n\\n\\n')\n",
    "#     pprint(GPTTokenizer.decode_batch(batch['y'].numpy()))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading a pre-trained model weights\n",
    "from dataset import get_dataloader, GPTTokenizer\n",
    "from model import (GPTModel, inference, token_ids_to_text, text_to_token_ids, generate)\n",
    "from config import CUSTOM_GPT_CONFIG\n",
    "from torchinfo import summary\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from rich import print as pprint\n",
    "from torch import tensor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "GPTModel                                 --\n",
       "â”œâ”€Embedding: 1-1                         38,597,376\n",
       "â”œâ”€Embedding: 1-2                         786,432\n",
       "â”œâ”€Dropout: 1-3                           --\n",
       "â”œâ”€Sequential: 1-4                        --\n",
       "â”‚    â””â”€TransformerBlock: 2-1             --\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention: 3-1      2,362,368\n",
       "â”‚    â”‚    â””â”€FeedForward: 3-2             4,722,432\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-3               1,536\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-4               1,536\n",
       "â”‚    â”‚    â””â”€Dropout: 3-5                 --\n",
       "â”‚    â””â”€TransformerBlock: 2-2             --\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention: 3-6      2,362,368\n",
       "â”‚    â”‚    â””â”€FeedForward: 3-7             4,722,432\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-8               1,536\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-9               1,536\n",
       "â”‚    â”‚    â””â”€Dropout: 3-10                --\n",
       "â”‚    â””â”€TransformerBlock: 2-3             --\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention: 3-11     2,362,368\n",
       "â”‚    â”‚    â””â”€FeedForward: 3-12            4,722,432\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-13              1,536\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-14              1,536\n",
       "â”‚    â”‚    â””â”€Dropout: 3-15                --\n",
       "â”‚    â””â”€TransformerBlock: 2-4             --\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention: 3-16     2,362,368\n",
       "â”‚    â”‚    â””â”€FeedForward: 3-17            4,722,432\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-18              1,536\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-19              1,536\n",
       "â”‚    â”‚    â””â”€Dropout: 3-20                --\n",
       "â”‚    â””â”€TransformerBlock: 2-5             --\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention: 3-21     2,362,368\n",
       "â”‚    â”‚    â””â”€FeedForward: 3-22            4,722,432\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-23              1,536\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-24              1,536\n",
       "â”‚    â”‚    â””â”€Dropout: 3-25                --\n",
       "â”‚    â””â”€TransformerBlock: 2-6             --\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention: 3-26     2,362,368\n",
       "â”‚    â”‚    â””â”€FeedForward: 3-27            4,722,432\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-28              1,536\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-29              1,536\n",
       "â”‚    â”‚    â””â”€Dropout: 3-30                --\n",
       "â”‚    â””â”€TransformerBlock: 2-7             --\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention: 3-31     2,362,368\n",
       "â”‚    â”‚    â””â”€FeedForward: 3-32            4,722,432\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-33              1,536\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-34              1,536\n",
       "â”‚    â”‚    â””â”€Dropout: 3-35                --\n",
       "â”‚    â””â”€TransformerBlock: 2-8             --\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention: 3-36     2,362,368\n",
       "â”‚    â”‚    â””â”€FeedForward: 3-37            4,722,432\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-38              1,536\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-39              1,536\n",
       "â”‚    â”‚    â””â”€Dropout: 3-40                --\n",
       "â”‚    â””â”€TransformerBlock: 2-9             --\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention: 3-41     2,362,368\n",
       "â”‚    â”‚    â””â”€FeedForward: 3-42            4,722,432\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-43              1,536\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-44              1,536\n",
       "â”‚    â”‚    â””â”€Dropout: 3-45                --\n",
       "â”‚    â””â”€TransformerBlock: 2-10            --\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention: 3-46     2,362,368\n",
       "â”‚    â”‚    â””â”€FeedForward: 3-47            4,722,432\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-48              1,536\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-49              1,536\n",
       "â”‚    â”‚    â””â”€Dropout: 3-50                --\n",
       "â”‚    â””â”€TransformerBlock: 2-11            --\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention: 3-51     2,362,368\n",
       "â”‚    â”‚    â””â”€FeedForward: 3-52            4,722,432\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-53              1,536\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-54              1,536\n",
       "â”‚    â”‚    â””â”€Dropout: 3-55                --\n",
       "â”‚    â””â”€TransformerBlock: 2-12            --\n",
       "â”‚    â”‚    â””â”€MultiHeadAttention: 3-56     2,362,368\n",
       "â”‚    â”‚    â””â”€FeedForward: 3-57            4,722,432\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-58              1,536\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-59              1,536\n",
       "â”‚    â”‚    â””â”€Dropout: 3-60                --\n",
       "â”œâ”€LayerNorm: 1-5                         1,536\n",
       "â”œâ”€Linear: 1-6                            38,597,376\n",
       "=================================================================\n",
       "Total params: 163,037,184\n",
       "Trainable params: 163,037,184\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst_model = GPTModel(CUSTOM_GPT_CONFIG)\n",
    "summary(inst_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: downloaded_weights/124M/checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: downloaded_weights/124M/encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: downloaded_weights/124M/hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: downloaded_weights/124M/model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: downloaded_weights/124M/model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: downloaded_weights/124M/model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravnaik/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: downloaded_weights/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "model_size = \"124M\"\n",
    "from gpt_download import download_and_load_gpt2\n",
    "model_dir = \"downloaded_weights\"\n",
    "settings, params = download_and_load_gpt2(model_size=model_size,\n",
    "                                          models_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(inst_model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['''Twinkle Twinkle little star\n",
    "How I wonder what you are\n",
    "Up above the world so high\n",
    "And now it takes a funny turn''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Response <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> : \n",
       "\n",
       "Twinkle Twinkle little star\n",
       "How I wonder what you are\n",
       "Up above the world so high\n",
       "And now it takes a funny turn\n",
       "So my face gets on top of the world\n",
       "So you could tell me how you like myself\n",
       "I'm on fire\n",
       "So you could tell me how you like myself I'm on fire I'm on fire How I wonder what you are Up above the world so \n",
       "highAnd now it takes a funny turnSo your face gets on top of the world So you could tell me how you like myself\n",
       "\n",
       "Lyrics\n",
       "\n",
       "You see, I am right on this little rock\n",
       "\n",
       "You see, you see\n",
       "\n",
       "I am on fire\n",
       "\n",
       "Lyrics\n",
       "\n",
       "You see I'm on fire\n",
       "\n",
       "I am on fire\n",
       "\n",
       "This rock's mine\n",
       "\n",
       "What I thought I was doing\n",
       "\n",
       "What I was going to do\n",
       "\n",
       "Just because I see all your faces\n",
       "\n",
       "It does not matter\n",
       "\n",
       "How I feel. You see, you see\n",
       "\n",
       "I am on fire\n",
       "\n",
       "I am on fire\n",
       "\n",
       "Lyrics\n",
       "\n",
       "Just so I'm in it,\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Response \u001b[1;36m1\u001b[0m : \n",
       "\n",
       "Twinkle Twinkle little star\n",
       "How I wonder what you are\n",
       "Up above the world so high\n",
       "And now it takes a funny turn\n",
       "So my face gets on top of the world\n",
       "So you could tell me how you like myself\n",
       "I'm on fire\n",
       "So you could tell me how you like myself I'm on fire I'm on fire How I wonder what you are Up above the world so \n",
       "highAnd now it takes a funny turnSo your face gets on top of the world So you could tell me how you like myself\n",
       "\n",
       "Lyrics\n",
       "\n",
       "You see, I am right on this little rock\n",
       "\n",
       "You see, you see\n",
       "\n",
       "I am on fire\n",
       "\n",
       "Lyrics\n",
       "\n",
       "You see I'm on fire\n",
       "\n",
       "I am on fire\n",
       "\n",
       "This rock's mine\n",
       "\n",
       "What I thought I was doing\n",
       "\n",
       "What I was going to do\n",
       "\n",
       "Just because I see all your faces\n",
       "\n",
       "It does not matter\n",
       "\n",
       "How I feel. You see, you see\n",
       "\n",
       "I am on fire\n",
       "\n",
       "I am on fire\n",
       "\n",
       "Lyrics\n",
       "\n",
       "Just so I'm in it,\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prompts = [\"I am the ONE\"] * 1\n",
    "inst_model.eval()\n",
    "res =    generate(model=inst_model,\n",
    "         tokenizer=GPTTokenizer,\n",
    "         max_new_tokens=200,\n",
    "         temperature= 1,\n",
    "         DEVICE = 'cpu',\n",
    "         prompts=prompts,\n",
    "         context_size=CUSTOM_GPT_CONFIG['context_length'],\n",
    "         top_K= 30,\n",
    "         eos_id=\"<|endoftext|>\")\n",
    "\n",
    "for idx, response in enumerate(res,1):\n",
    "    pprint(f\"Response {idx} : \\n\\n{response}\", end = '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,optimizer, train_loader, epochs = 10, DEVICE = 'cpu'):\n",
    "    # optimizer = optim.AdamW(model.parameters(),\n",
    "    #                         lr = 0.01)\n",
    "    tokens_seen = 0\n",
    "    losses = []\n",
    "    EPOCHS = epochs\n",
    "    # epoch_pbar = tqdm(range(EPOCHS), desc=\"Training\", unit=\"epoch\")\n",
    "    # epoch_pbar = tqdm(range(EPOCHS), desc=\"Training\", unit=\"epoch\")\n",
    "    model.to(DEVICE)\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        batch_loss = 0\n",
    "\n",
    "\n",
    "        batch_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "\n",
    "        for batch in batch_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            out_logits = model(batch['x'].to(DEVICE))\n",
    "            loss = F.cross_entropy(out_logits.flatten(0,1), batch['y'].flatten(0).to(DEVICE))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += batch['x'].numel()\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "\n",
    "        avg_loss = batch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"EPOCH : {epoch + 1} | Epoch Loss : {losses[epoch]}\")\n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 1 | Epoch Loss : 0.32288456489058104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 2 | Epoch Loss : 0.2822493472520043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 3 | Epoch Loss : 0.2525990780662088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# optimizer = optim.AdamW(lr=0.001,weight_decay=0.01, params=inst_model.parameters())\n",
    "inst_model, optimizer, losses = train_model(model = inst_model,optimizer=optimizer,\n",
    "            train_loader=train_loader, epochs = 3, DEVICE = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and optimizer state\n",
    "checkpoint = {\n",
    "    'model_state_dict': inst_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'saved_models/instruction_model_160M_7EP.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_, res = format_input(test_data.loc[1090])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Below is an instruction that describes the task. Write a response that appropriately completes the request\n",
       "\n",
       "### Instruction:\n",
       "Provide a synonym for <span style=\"color: #008000; text-decoration-color: #008000\">'beautiful'</span>.\n",
       "\n",
       "### Response:\n",
       " A synonym for <span style=\"color: #008000; text-decoration-color: #008000\">'beautiful'</span> is <span style=\"color: #008000; text-decoration-color: #008000\">'gorgeous'</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Below is an instruction that describes the task. Write a response that appropriately completes the request\n",
       "\n",
       "### Instruction:\n",
       "Provide a synonym for \u001b[32m'beautiful'\u001b[0m.\n",
       "\n",
       "### Response:\n",
       " A synonym for \u001b[32m'beautiful'\u001b[0m is \u001b[32m'gorgeous'\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(inp_, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Below is an instruction that describes the task. Write a response that appropriately completes the request\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "'''\n",
    "\n",
    "def generate_prompt(instruction, input_text=\"\"):\n",
    "    return template.format(instruction=instruction, input=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke(model, tokenizer, instruction, input_text):\n",
    "    model.eval()\n",
    "    prompt = [generate_prompt(instruction, input_text=None)]\n",
    "    generated_responses =    generate(model=inst_model,\n",
    "             tokenizer=GPTTokenizer,\n",
    "             max_new_tokens=90,\n",
    "             temperature= 0,\n",
    "             DEVICE = 'cpu',\n",
    "             prompts=prompt,\n",
    "             context_size=CUSTOM_GPT_CONFIG['context_length'],\n",
    "             top_K= 20,\n",
    "             eos_id=\"<|endoftext|>\")\n",
    "    responses = []\n",
    "    for i,res in enumerate(generated_responses):\n",
    "        responses.append(res[res.find(\"### Response\") + len(\"### Response:\"):res.find(\"<|endoftext|>\")])\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "The simile is a figure of speech.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "The simile is a figure of speech.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ALPACA FORMAT\n",
    "'''Below is an instruction that describes the task. Write a response that appropriately completes the request\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "'''\n",
    "response = invoke(inst_model,\n",
    "       GPTTokenizer,\n",
    "       instruction=\"Rewrite the sentence using a simile.\",\n",
    "       input_text= \"The dog is very loyal.\"\n",
    ")\n",
    "pprint(response[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
